{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f743288f-2b62-4d19-a55b-933ebbcdcb17",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- This notebook fine-tunes the open-source models released by Google (gemma). The notebook can finetune both the 2b and 7b models.\n",
    "- Finetuned on an A10G GPU\n",
    "- Lora is used for finetuning.\n",
    "- Dataset is Financial Phrase Bank from Kaggle.\n",
    "- Inspired from (https://www.kaggle.com/code/lucamassaron/fine-tune-gemma-7b-it-for-sentiment-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7cf26f-41f8-40ce-b823-b8a2d499e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from watermark import watermark\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # environment variable which tells pytorch to use the first GPU\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Whether to parallelize tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3b63a1-8a27-4ccf-bbd6-12ee37ccf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1617386d-87f9-4e6e-aa7b-a657901e0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16bbbd8e-544a-4bcd-a692-4075dbe836e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.2.1\n",
      "transformers: 4.38.1\n",
      "datasets    : 2.17.1\n",
      "accelerate  : 0.27.2\n",
      "trl         : 0.7.11\n",
      "peft        : 0.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p torch,transformers,datasets,accelerate,trl,peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adad34d-ff2e-4c6c-9019-e0ee28b0a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file name.\n",
    "filename = \"all-data.csv\"\n",
    "\n",
    "# Read the file to pandas\n",
    "df = pd.read_csv(filename, \n",
    "                 names=[\"sentiment\", \"text\"],\n",
    "                 encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "# Create train, and test\n",
    "X_train = list()\n",
    "X_test = list()\n",
    "\n",
    "# sample from each sentiment randomly and add 300 each to train and test.\n",
    "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    train, test  = train_test_split(df[df.sentiment==sentiment], \n",
    "                                    train_size=300,\n",
    "                                    test_size=300, \n",
    "                                    random_state=42)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "\n",
    "# The ones that are not used will be eval set (randomly sample 50 from each sentiment)\n",
    "eval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]\n",
    "X_eval = df[df.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Generate prompt for fine-tuning\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "# Generate prompt for test\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "            [{data_point[\"text\"]}] = \n",
    "\n",
    "            \"\"\".strip()\n",
    "\n",
    "# Prompt from gemma model card\n",
    "# def generate_prompt(data_point):\n",
    "#     return f\"\"\"<start_of_turn>user\n",
    "#             Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "#             determine if it is positive, neutral, or negative, and return the answer as \n",
    "#             the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "            \n",
    "#             [{data_point[\"text\"]}]<end_of_turn>\n",
    "#             <start_of_turn>model\n",
    "#             {data_point[\"sentiment\"]}<end_of_turn>\"\"\".strip()\n",
    "\n",
    "# def generate_test_prompt(data_point):\n",
    "#     return f\"\"\"<start_of_turn>user\n",
    "#             Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "#             determine if it is positive, neutral, or negative, and return the answer as \n",
    "#             the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "            \n",
    "#             [{data_point[\"text\"]}]<end_of_turn>\n",
    "#             <start_of_turn>model\n",
    "#             \"\"\".strip()\n",
    "\n",
    "# Conver train and eval to Datasets\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bb6f1d-781c-435f-ae8e-16df62d7d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e6f5e-bfd0-4f55-8156-8fc322e1213b",
   "metadata": {},
   "source": [
    "## Function to evaluate the performs of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88be0978-2e75-4333-903d-74856f1af222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Maps the labels to positive(2) , negative(0) or neutral(1).\n",
    "    Calculate the overall accuracy.\n",
    "    Calculate the overall accuracy of individual labels.\n",
    "    Also generate classification report and confusion matrix.\n",
    "    \"\"\"\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122752f-b173-4387-8cbd-901415ba432c",
   "metadata": {},
   "source": [
    "# Load the model and tokensizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85434eff-264e-4758-a821-3a5460796201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1453b8c6e49a99c3f8b35be5943c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dcdbb61e9a49d9820c496f12dc8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44c5243aed04baf9542f8d85debcdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed0c442b3d947d988a15dc99c980fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d005ca50c984bfe9f9a00042aeab1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860554edd5784dd1b9e4f7204691aaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9bbc48014c4f788d4c097d3b375555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6659de1c4eb04441a73f4ae02517a731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0073216c0f7b4b65b3128d1e51a073b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d9dc449d0448a0876d3efcf1e00031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c21d9c9504498fb2a7bec1a428c3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af71826c48e486e94057d03bbb76e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaa9d7beb5041559dfae9c9468209e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # load in 4bit\n",
    "    bnb_4bit_use_double_quant=False, # load in double quant\n",
    "    bnb_4bit_quant_type=\"nf4\", # nf4 type of quantization\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # use float16 for computation.\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d815c-0409-427c-abbb-5f457838d2f8",
   "metadata": {},
   "source": [
    "## Make predictions using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16dae022-7739-45bb-998f-6203936f2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate predictions. Input prompt is first converted to tokens,\n",
    "    The tokens are sent to the LLM to generate an output.\n",
    "    The output is then mapped to positive, negative, neutral\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids, max_new_tokens=1, temperature=0.0)\n",
    "        result = tokenizer.decode(outputs[0])\n",
    "        answer = result.split(\"=\")[-1].lower()\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38dd1cee-4af2-4daf-8987-44eac8a71bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 900/900 [01:33<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.631\n",
      "Accuracy for label 0: 0.803\n",
      "Accuracy for label 1: 0.193\n",
      "Accuracy for label 2: 0.897\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86       300\n",
      "           1       0.46      0.19      0.27       300\n",
      "           2       0.52      0.90      0.66       300\n",
      "\n",
      "    accuracy                           0.63       900\n",
      "   macro avg       0.64      0.63      0.60       900\n",
      "weighted avg       0.64      0.63      0.60       900\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[241  40  19]\n",
      " [ 13  58 229]\n",
      " [  4  27 269]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e334fdf-6d20-4d6a-8380-1c2e7c11cee2",
   "metadata": {},
   "source": [
    "### The accuracy across all the sentiments is 0.631. The accuracy is the best for label 2 and worst for the label 1.\n",
    "- This is very different from the gemma_2b_it which had good accuracy on label 0 and worst on label 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e27864-6daa-4a39-ab55-6d16686a4ad2",
   "metadata": {},
   "source": [
    "# Finetuning the model\n",
    "- Using the SFTTrainer.\n",
    "- We will use the PEFT method, and set the parameters for LORA.\n",
    "- Set the training parameters such as logging, number of epoch, batch_size, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5056a9f-5aa1-4d80-893e-3780cb8fe511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b2a11ca0d14487a48b56d377595de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.326, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    do_eval=False,\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d75fde-82ff-4a70-9bf5-e8ef3d43b38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 12:46, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=336, training_loss=0.9918200472990671, metrics={'train_runtime': 769.96, 'train_samples_per_second': 3.507, 'train_steps_per_second': 0.436, 'total_flos': 1.124157764164608e+16, 'train_loss': 0.9918200472990671, 'epoch': 2.99})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model \n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "# trainer.model.save_pretrained(\"trained-model_it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d389dbe-9378-4e7d-a13c-e9262232d149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/runs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b02eb2-8651-44b5-9367-ee9c1f5f1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs/runs (started 0:00:26 ago; pid 14175)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "print(notebook.list()) # View open TensorBoard instances\n",
    "# !kill 14175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b1b4b-48ce-40b4-b4c7-2f963c3715a9",
   "metadata": {},
   "source": [
    "# Evaluate the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbcf5c3e-d724-488c-897a-89fc835efdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 900/900 [02:10<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.886\n",
      "Accuracy for label 0: 0.977\n",
      "Accuracy for label 1: 0.877\n",
      "Accuracy for label 2: 0.803\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       300\n",
      "           1       0.81      0.88      0.84       300\n",
      "           2       0.90      0.80      0.85       300\n",
      "\n",
      "    accuracy                           0.89       900\n",
      "   macro avg       0.89      0.89      0.89       900\n",
      "weighted avg       0.89      0.89      0.89       900\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[293   4   3]\n",
      " [ 12 263  25]\n",
      " [  2  57 241]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7870fc-c2ab-480d-9521-f07b00e98ce5",
   "metadata": {},
   "source": [
    "### After training the performance improves significantly\n",
    "- Overall accuracy is 0.886, and a little better than gemma-2b-it\n",
    "- And the model performs well on all the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d644-7796-42bb-b839-cc97cab58179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
