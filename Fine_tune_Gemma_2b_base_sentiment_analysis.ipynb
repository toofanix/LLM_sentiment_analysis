{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f743288f-2b62-4d19-a55b-933ebbcdcb17",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- This notebook fine-tunes the open-source models released by Google (gemma). The notebook can finetune both the 2b and 7b models.\n",
    "- Finetuned on an A10G GPU\n",
    "- Lora is used for finetuning.\n",
    "- Dataset is Financial Phrase Bank from Kaggle.\n",
    "- Inspired from (https://www.kaggle.com/code/lucamassaron/fine-tune-gemma-7b-it-for-sentiment-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7cf26f-41f8-40ce-b823-b8a2d499e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from watermark import watermark\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # environment variable which tells pytorch to use the first GPU\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Whether to parallelize tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3b63a1-8a27-4ccf-bbd6-12ee37ccf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1617386d-87f9-4e6e-aa7b-a657901e0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16bbbd8e-544a-4bcd-a692-4075dbe836e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.2.1\n",
      "transformers: 4.38.1\n",
      "datasets    : 2.17.1\n",
      "accelerate  : 0.27.2\n",
      "trl         : 0.7.11\n",
      "peft        : 0.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p torch,transformers,datasets,accelerate,trl,peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adad34d-ff2e-4c6c-9019-e0ee28b0a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file name.\n",
    "filename = \"all-data.csv\"\n",
    "\n",
    "# Read the file to pandas\n",
    "df = pd.read_csv(filename, \n",
    "                 names=[\"sentiment\", \"text\"],\n",
    "                 encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "# Create train, and test\n",
    "X_train = list()\n",
    "X_test = list()\n",
    "\n",
    "# sample from each sentiment randomly and add 300 each to train and test.\n",
    "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    train, test  = train_test_split(df[df.sentiment==sentiment], \n",
    "                                    train_size=300,\n",
    "                                    test_size=300, \n",
    "                                    random_state=42)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "\n",
    "# The ones that are not used will be eval set (randomly sample 50 from each sentiment)\n",
    "eval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]\n",
    "X_eval = df[df.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Generate prompt for fine-tuning\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "# Generate prompt for test\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "            [{data_point[\"text\"]}] = \n",
    "\n",
    "            \"\"\".strip()\n",
    "\n",
    "# Prompt from gemma model card\n",
    "# def generate_prompt(data_point):\n",
    "#     return f\"\"\"<start_of_turn>user\n",
    "#             Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "#             determine if it is positive, neutral, or negative, and return the answer as \n",
    "#             the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "            \n",
    "#             [{data_point[\"text\"]}]<end_of_turn>\n",
    "#             <start_of_turn>model\n",
    "#             {data_point[\"sentiment\"]}<end_of_turn>\"\"\".strip()\n",
    "\n",
    "# def generate_test_prompt(data_point):\n",
    "#     return f\"\"\"<start_of_turn>user\n",
    "#             Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "#             determine if it is positive, neutral, or negative, and return the answer as \n",
    "#             the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "            \n",
    "#             [{data_point[\"text\"]}]<end_of_turn>\n",
    "#             <start_of_turn>model\n",
    "#             \"\"\".strip()\n",
    "\n",
    "# Conver train and eval to Datasets\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bb6f1d-781c-435f-ae8e-16df62d7d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e6f5e-bfd0-4f55-8156-8fc322e1213b",
   "metadata": {},
   "source": [
    "## Function to evaluate the performs of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88be0978-2e75-4333-903d-74856f1af222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Maps the labels to positive(2) , negative(0) or neutral(1).\n",
    "    Calculate the overall accuracy.\n",
    "    Calculate the overall accuracy of individual labels.\n",
    "    Also generate classification report and confusion matrix.\n",
    "    \"\"\"\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122752f-b173-4387-8cbd-901415ba432c",
   "metadata": {},
   "source": [
    "# Load the model and tokensizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85434eff-264e-4758-a821-3a5460796201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7931ce3086d4ef0a6f45000c5116c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd932bdcdf3f4baeb2ff4b94a1c39cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ff810fb7e348de98041bb5e67e5958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b358d4e5a114b10853f6e56d4b946c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402ec8ed4c194350816acb76bd7222c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ae310e32c14b05bdfaed2873c13b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc89504bc234ed89d96313752f3d8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3feb466e26646789df3890d04d3df85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d0b6022415409d9ffd953501263dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58484fcb6ad84827a05d02a6ddad042d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6218bdf25964f1f9e26c46cedad8e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # load in 4bit\n",
    "    bnb_4bit_use_double_quant=False, # load in double quant\n",
    "    bnb_4bit_quant_type=\"nf4\", # nf4 type of quantization\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # use float16 for computation.\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d815c-0409-427c-abbb-5f457838d2f8",
   "metadata": {},
   "source": [
    "## Make predictions using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16dae022-7739-45bb-998f-6203936f2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate predictions. Input prompt is first converted to tokens,\n",
    "    The tokens are sent to the LLM to generate an output.\n",
    "    The output is then mapped to positive, negative, neutral\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids, max_new_tokens=1, temperature=0.0)\n",
    "        result = tokenizer.decode(outputs[0])\n",
    "        answer = result.split(\"=\")[-1].lower()\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38dd1cee-4af2-4daf-8987-44eac8a71bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:38<00:00, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.564\n",
      "Accuracy for label 0: 0.747\n",
      "Accuracy for label 1: 0.357\n",
      "Accuracy for label 2: 0.590\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.76       300\n",
      "           1       0.39      0.36      0.37       300\n",
      "           2       0.53      0.59      0.56       300\n",
      "\n",
      "    accuracy                           0.56       900\n",
      "   macro avg       0.56      0.56      0.56       900\n",
      "weighted avg       0.56      0.56      0.56       900\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[224  69   7]\n",
      " [ 42 107 151]\n",
      " [ 23 100 177]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e334fdf-6d20-4d6a-8380-1c2e7c11cee2",
   "metadata": {},
   "source": [
    "### The accuracy across all the sentiments is 0.564. The accuracy is decent across all the labels. This is different than the instructionn tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e27864-6daa-4a39-ab55-6d16686a4ad2",
   "metadata": {},
   "source": [
    "# Finetuning the model\n",
    "- Using the SFTTrainer.\n",
    "- We will use the PEFT method, and set the parameters for LORA.\n",
    "- Set the training parameters such as logging, number of epoch, batch_size, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5056a9f-5aa1-4d80-893e-3780cb8fe511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bd91239aa94307a0a207ab50d97f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.326, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    do_eval=False,\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d75fde-82ff-4a70-9bf5-e8ef3d43b38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 06:57, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.956400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.790900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.663700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=336, training_loss=0.8815730398609525, metrics={'train_runtime': 419.3484, 'train_samples_per_second': 6.439, 'train_steps_per_second': 0.801, 'total_flos': 2912916582420480.0, 'train_loss': 0.8815730398609525, 'epoch': 2.99})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model \n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "# trainer.model.save_pretrained(\"trained-model_it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d389dbe-9378-4e7d-a13c-e9262232d149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/runs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b02eb2-8651-44b5-9367-ee9c1f5f1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs/runs (started 0:00:20 ago; pid 16147)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "print(notebook.list()) # View open TensorBoard instances\n",
    "# !kill 16147"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b1b4b-48ce-40b4-b4c7-2f963c3715a9",
   "metadata": {},
   "source": [
    "# Evaluate the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbcf5c3e-d724-488c-897a-89fc835efdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 900/900 [01:04<00:00, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.847\n",
      "Accuracy for label 0: 0.913\n",
      "Accuracy for label 1: 0.843\n",
      "Accuracy for label 2: 0.783\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       300\n",
      "           1       0.76      0.84      0.80       300\n",
      "           2       0.84      0.78      0.81       300\n",
      "\n",
      "    accuracy                           0.85       900\n",
      "   macro avg       0.85      0.85      0.85       900\n",
      "weighted avg       0.85      0.85      0.85       900\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[274  20   6]\n",
      " [  8 253  39]\n",
      " [  4  61 235]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7870fc-c2ab-480d-9521-f07b00e98ce5",
   "metadata": {},
   "source": [
    "### After training the performance improves significantly\n",
    "- Overall accuracy is 0.847\n",
    "- And the model performs well on all the labels.\n",
    "- The model is not better than gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d644-7796-42bb-b839-cc97cab58179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
